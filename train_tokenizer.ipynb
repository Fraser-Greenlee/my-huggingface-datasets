{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset mnist_text (/Users/frasergreenlee/.cache/huggingface/datasets/mnist_text/default/0.0.0/a3119a338290d44106fc69ba1fe30002da4a257185dc61d966d848d98aa05b7c)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('Fraser/mnist-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:03<00:00, 33144.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open('cache/train_tokenizer.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "txt = []\n",
    "for r in tqdm(dataset['train']):\n",
    "    txt.append(r['text'].replace(' ', ''))\n",
    "\n",
    "with open('cache/train_tokenizer.txt', 'a') as f:\n",
    "    f.write('\\n'.join(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ByteLevelBPETokenizer in module tokenizers.implementations.byte_level_bpe:\n",
      "\n",
      "class ByteLevelBPETokenizer(tokenizers.implementations.base_tokenizer.BaseTokenizer)\n",
      " |  ByteLevelBPETokenizer(vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, add_prefix_space: bool = False, lowercase: bool = False, dropout: Union[float, NoneType] = None, unicode_normalizer: Union[str, NoneType] = None, continuing_subword_prefix: Union[str, NoneType] = None, end_of_word_suffix: Union[str, NoneType] = None, trim_offsets: bool = False)\n",
      " |  \n",
      " |  ByteLevelBPETokenizer\n",
      " |  \n",
      " |  Represents a Byte-level BPE as introduced by OpenAI with their GPT-2 model\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ByteLevelBPETokenizer\n",
      " |      tokenizers.implementations.base_tokenizer.BaseTokenizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, add_prefix_space: bool = False, lowercase: bool = False, dropout: Union[float, NoneType] = None, unicode_normalizer: Union[str, NoneType] = None, continuing_subword_prefix: Union[str, NoneType] = None, end_of_word_suffix: Union[str, NoneType] = None, trim_offsets: bool = False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  train(self, files: Union[str, List[str]], vocab_size: int = 30000, min_frequency: int = 2, show_progress: bool = True, special_tokens: List[Union[str, tokenizers.AddedToken]] = [])\n",
      " |      Train the model using the given files\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_file(vocab_filename: str, merges_filename: str, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add the given special tokens to the vocabulary, and treat them as special tokens.\n",
      " |      \n",
      " |      The special tokens will never be processed by the model, and will be\n",
      " |      removed while decoding.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: List[Union[str, AddedToken]]:\n",
      " |              A list of special tokens to add to the vocabulary. Each token can either be\n",
      " |              a string, or an instance of AddedToken\n",
      " |      \n",
      " |      Returns:\n",
      " |          The number of tokens that were added to the vocabulary\n",
      " |  \n",
      " |  add_tokens(self, tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add the given tokens to the vocabulary\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: List[Union[str, AddedToken]]:\n",
      " |              A list of tokens to add to the vocabulary. Each token can either be\n",
      " |              a string, or an instance of AddedToken\n",
      " |      \n",
      " |      Returns:\n",
      " |          The number of tokens that were added to the vocabulary\n",
      " |  \n",
      " |  decode(self, ids: List[int], skip_special_tokens: Union[bool, NoneType] = True) -> str\n",
      " |      Decode the given list of ids to a string sequence\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List[unsigned int]:\n",
      " |              A list of ids to be decoded\n",
      " |      \n",
      " |          skip_special_tokens: (`optional`) boolean:\n",
      " |              Whether to remove all the special tokens from the output string\n",
      " |      \n",
      " |      Returns:\n",
      " |          The decoded string\n",
      " |  \n",
      " |  decode_batch(self, sequences: List[List[int]], skip_special_tokens: Union[bool, NoneType] = True) -> str\n",
      " |      Decode the list of sequences to a list of string sequences\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences: List[List[unsigned int]]:\n",
      " |              A list of sequence of ids to be decoded\n",
      " |      \n",
      " |          skip_special_tokens: (`optional`) boolean:\n",
      " |              Whether to remove all the special tokens from the output strings\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of decoded strings\n",
      " |  \n",
      " |  enable_padding(self, direction: Union[str, NoneType] = 'right', pad_to_multiple_of: Union[int, NoneType] = None, pad_id: Union[int, NoneType] = 0, pad_type_id: Union[int, NoneType] = 0, pad_token: Union[str, NoneType] = '[PAD]', length: Union[int, NoneType] = None)\n",
      " |      Change the padding strategy\n",
      " |      \n",
      " |      Args:\n",
      " |          direction: (`optional`) str:\n",
      " |              Can be one of: `right` or `left`\n",
      " |      \n",
      " |          pad_to_multiple_of: (`optional`) unsigned int:\n",
      " |              If specified, the padding length should always snap to the next multiple of\n",
      " |              the given value. For example if we were going to pad with a length of 250 but\n",
      " |              `pad_to_multiple_of=8` then we will pad to 256.\n",
      " |      \n",
      " |          pad_id: (`optional`) unsigned int:\n",
      " |              The indice to be used when padding\n",
      " |      \n",
      " |          pad_type_id: (`optional`) unsigned int:\n",
      " |              The type indice to be used when padding\n",
      " |      \n",
      " |          pad_token: (`optional`) str:\n",
      " |              The pad token to be used when padding\n",
      " |      \n",
      " |          length: (`optional`) unsigned int:\n",
      " |              If specified, the length at which to pad. If not specified\n",
      " |              we pad using the size of the longest sequence in a batch\n",
      " |  \n",
      " |  enable_truncation(self, max_length: int, stride: Union[int, NoneType] = 0, strategy: Union[str, NoneType] = 'longest_first')\n",
      " |      Change the truncation options\n",
      " |      \n",
      " |      Args:\n",
      " |          max_length: unsigned int:\n",
      " |              The maximum length at which to truncate\n",
      " |      \n",
      " |          stride: (`optional`) unsigned int:\n",
      " |              The length of the previous first sequence to be included\n",
      " |              in the overflowing sequence\n",
      " |      \n",
      " |          strategy: (`optional) str:\n",
      " |              Can be one of `longest_first`, `only_first` or `only_second`\n",
      " |  \n",
      " |  encode(self, sequence: Union[str, List[str], Tuple[str]], pair: Union[str, List[str], Tuple[str], NoneType] = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> tokenizers.Encoding\n",
      " |      Encode the given sequence and pair. This method can process raw text sequences as well\n",
      " |      as already pre-tokenized sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequence: InputSequence:\n",
      " |              The sequence we want to encode. This sequence can be either raw text or\n",
      " |              pre-tokenized, according to the `is_pretokenized` argument:\n",
      " |      \n",
      " |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n",
      " |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n",
      " |                  `Union[List[str], Tuple[str]]`\n",
      " |      \n",
      " |          is_pretokenized: bool:\n",
      " |              Whether the input is already pre-tokenized.\n",
      " |      \n",
      " |          add_special_tokens: bool:\n",
      " |              Whether to add the special tokens while encoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An Encoding\n",
      " |  \n",
      " |  encode_batch(self, inputs: List[Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]], is_pretokenized: bool = False, add_special_tokens: bool = True) -> List[tokenizers.Encoding]\n",
      " |      Encode the given inputs. This method accept both raw text sequences as well as already\n",
      " |      pre-tokenized sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: List[EncodeInput]:\n",
      " |              A list of single sequences or pair sequences to encode. Each `EncodeInput` is\n",
      " |              expected to be of the following form:\n",
      " |                  `Union[InputSequence, Tuple[InputSequence, InputSequence]]`\n",
      " |      \n",
      " |              Each `InputSequence` can either be raw text or pre-tokenized,\n",
      " |              according to the `is_pretokenized` argument:\n",
      " |      \n",
      " |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n",
      " |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n",
      " |                  `Union[List[str], Tuple[str]]`\n",
      " |      \n",
      " |          is_pretokenized: bool:\n",
      " |              Whether the input is already pre-tokenized.\n",
      " |      \n",
      " |          add_special_tokens: bool:\n",
      " |              Whether to add the special tokens while encoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of Encoding\n",
      " |  \n",
      " |  get_vocab(self, with_added_tokens: bool = True) -> Dict[str, int]\n",
      " |      Returns the vocabulary\n",
      " |      \n",
      " |      Args:\n",
      " |          with_added_tokens: boolean:\n",
      " |              Whether to include the added tokens in the vocabulary\n",
      " |      \n",
      " |      Returns:\n",
      " |          The vocabulary\n",
      " |  \n",
      " |  get_vocab_size(self, with_added_tokens: bool = True) -> int\n",
      " |      Return the size of vocabulary, with or without added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          with_added_tokens: (`optional`) bool:\n",
      " |              Whether to count in added special tokens or not\n",
      " |      \n",
      " |      Returns:\n",
      " |          Size of vocabulary\n",
      " |  \n",
      " |  id_to_token(self, id: int) -> Union[str, NoneType]\n",
      " |      Convert the given token id to its corresponding string\n",
      " |      \n",
      " |      Args:\n",
      " |          token: id:\n",
      " |              The token id to convert\n",
      " |      \n",
      " |      Returns:\n",
      " |          The corresponding string if it exists, None otherwise\n",
      " |  \n",
      " |  no_padding(self)\n",
      " |      Disable padding\n",
      " |  \n",
      " |  no_truncation(self)\n",
      " |      Disable truncation\n",
      " |  \n",
      " |  normalize(self, sequence: str) -> str\n",
      " |      Normalize the given sequence\n",
      " |      \n",
      " |      Args:\n",
      " |          sequence: str:\n",
      " |              The sequence to normalize\n",
      " |      \n",
      " |      Returns:\n",
      " |          The normalized string\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, is_pair: bool) -> int\n",
      " |      Return the number of special tokens that would be added for single/pair sentences.\n",
      " |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n",
      " |      :return:\n",
      " |  \n",
      " |  post_process(self, encoding: tokenizers.Encoding, pair: Union[tokenizers.Encoding, NoneType] = None, add_special_tokens: bool = True) -> tokenizers.Encoding\n",
      " |      Apply all the post-processing steps to the given encodings.\n",
      " |      \n",
      " |      The various steps are:\n",
      " |          1. Truncate according to global params (provided to `enable_truncation`)\n",
      " |          2. Apply the PostProcessor\n",
      " |          3. Pad according to global params. (provided to `enable_padding`)\n",
      " |      \n",
      " |      Args:\n",
      " |          encoding: Encoding:\n",
      " |              The main Encoding to post process\n",
      " |      \n",
      " |          pair: Optional[Encoding]:\n",
      " |              An optional pair Encoding\n",
      " |      \n",
      " |          add_special_tokens: bool:\n",
      " |              Whether to add special tokens\n",
      " |      \n",
      " |      Returns:\n",
      " |          The resulting Encoding\n",
      " |  \n",
      " |  save(self, path: str, pretty: bool = False)\n",
      " |      Save the current Tokenizer at the given path\n",
      " |      \n",
      " |      Args:\n",
      " |          path: str:\n",
      " |              A path to the destination Tokenizer file\n",
      " |  \n",
      " |  save_model(self, directory: str, name: Union[str, NoneType] = None)\n",
      " |      Save the current model to the given directory\n",
      " |      \n",
      " |      Args:\n",
      " |          directory: str:\n",
      " |              A path to the destination directory\n",
      " |      \n",
      " |          name: (Optional) str:\n",
      " |              The name of the tokenizer, to be used in the saved files\n",
      " |  \n",
      " |  to_str(self, pretty: bool = False)\n",
      " |      Get a serialized JSON version of the Tokenizer as a str\n",
      " |      \n",
      " |      Args:\n",
      " |          pretty: bool:\n",
      " |              Whether the JSON string should be prettified\n",
      " |      \n",
      " |      Returns:\n",
      " |          str\n",
      " |  \n",
      " |  token_to_id(self, token: str) -> Union[int, NoneType]\n",
      " |      Convert the given token to its corresponding id\n",
      " |      \n",
      " |      Args:\n",
      " |          token: str:\n",
      " |              The token to convert\n",
      " |      \n",
      " |      Returns:\n",
      " |          The corresponding id if it exists, None otherwise\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  padding\n",
      " |      Get the current padding parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          None if padding is disabled, a dict with the currently set parameters\n",
      " |          if the padding is enabled.\n",
      " |  \n",
      " |  truncation\n",
      " |      Get the current truncation parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          None if truncation is disabled, a dict with the current truncation parameters if\n",
      " |          truncation is enabled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ByteLevelBPETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, Tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(\n",
    "    'cache/train_tokenizer.txt',\n",
    "    vocab_size=32128,\n",
    "    min_frequency=10,\n",
    "    special_tokens=['</s>', '<pad>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=32128, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''\n",
    "00 down ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
    "01 down ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
    "02 down ! ! ! ! ! ! % % C L a ^ ! !\n",
    "03 down ! ! ! - ` ` ` ` ` Y ` Q ! !\n",
    "04 down ! ! ! % ` ` ` R ^ ! ! ! ! !\n",
    "05 down ! ! ! ! $ G ` ! ! ! ! ! ! !\n",
    "06 down ! ! ! ! ! # ` Y < ! ! ! ! !\n",
    "07 down ! ! ! ! ! ! 5 ` ` F ! ! ! !\n",
    "08 down ! ! ! ! ! ! ! % ` ` 1 ! ! !\n",
    "09 down ! ! ! ! ! ! F ` ` ` ! ! ! !\n",
    "10 down ! ! ! ! 1 ` ` ` ` 4 ! ! ! !\n",
    "11 down ! ! L ` ` ` ` 5 ! ! ! ! ! !\n",
    "12 down ! ! ` ` V B ! ! ! ! ! ! ! !\n",
    "13 down ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505, 224)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s), len(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['\\n', '00', 'down', '!!!!!!!!!!!!!!', '\\n', '01', 'down', '!!!!!!!!!!!!!!', '\\n', '02', 'down', '!!!!!!%', '%', 'CLa', '^!!', '\\n', '03', 'down', '!!!-', '`````', 'Y', '`', 'Q', '!!', '\\n', '04', 'down', '!!!%', '```', 'R', '^!!!!!', '\\n', '05', 'down', '!!!!$', 'G', '`!!!!!!!', '\\n', '06', 'down', '!!!!!#`', 'Y', '<!!!!!', '\\n', '07', 'down', '!!!!!!', '5', '``', 'F', '!!!!', '\\n', '08', 'down', '!!!!!!!%``', '1', '!!!', '\\n', '09', 'down', '!!!!!!', 'F', '```!!!!', '\\n', '10', 'down', '!!!!', '1', '````', '4', '!!!!', '\\n', '11', 'down', '!!', 'L', '````', '5', '!!!!!!', '\\n', '12', 'down', '!!``', 'VB', '!!!!!!!!', '\\n', '13', 'down', '!!!!!!!!!!!!!!', '\\n']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(s.replace(' ', '')).ids\n",
    "\n",
    "print(len(token_ids))\n",
    "print([tokenizer.decode([tkn_id]) for tkn_id in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['\\n', '00', 'down', '!!!!!!!!!!!!!!', '\\n', '01', 'down', '!!!!!!!!!!!!!!', '\\n', '02', 'down', '!!!!!!%', '%', 'CLa', '^!!', '\\n', '03', 'down', '!!!-', '`````', 'Y', '`', 'Q', '!!', '\\n', '04', 'down', '!!!%', '```', 'R', '^!!!!!', '\\n', '05', 'down', '!!!!$', 'G', '`!!!!!!!', '\\n', '06', 'down', '!!!!!#`', 'Y', '<!!!!!', '\\n', '07', 'down', '!!!!!!', '5', '``', 'F', '!!!!', '\\n', '08', 'down', '!!!!!!!%``', '1', '!!!', '\\n', '09', 'down', '!!!!!!', 'F', '```!!!!', '\\n', '10', 'down', '!!!!', '1', '````', '4', '!!!!', '\\n', '11', 'down', '!!', 'L', '````', '5', '!!!!!!', '\\n', '12', 'down', '!!``', 'VB', '!!!!!!!!', '\\n', '13', 'down', '!!!!!!!!!!!!!!', '\\n']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(s.replace(' ', '')).ids\n",
    "\n",
    "print(len(token_ids))\n",
    "print([tokenizer.decode([tkn_id]) for tkn_id in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n",
      "['27', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '26', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '25', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '24', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '23', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '22', 'up', '!!!!!$', '8', 'D', '?&', '\"!!!!!!!!!!!!!!!!!', '\\n', '21', 'up', '!!!!!', 'T', '````', 'M', '>*', '\"!!!!!!!!!!!!!!', '\\n', '20', 'up', '!!!!!', 'Z', '`^', 'TW', '```@!!!!!!!!!!!!!!', '\\n', '19', 'up', '!!!!!;', '`]', '7', '%', '3', 'K', '^`@', '(!!!!!', '7', 'NN', '*!!!', '\\n', '18', 'up', '!!!!!!', 'C', '`];', '%!.', 'S', '`', 'W', '1', '!$', '3', 'I', ']``', 'X', '!!!', '\\n', '17', 'up', '!!!!!!\"', 'B', '``', 'N', '%!&', 'S', '`^', 'AL', '``', 'O', '1', '--!!!', '\\n', '16', 'up', '!!!!!!!\"', '4', '``@', '/!%', 'U', '````', '6', '\"!!!!!!', '\\n', '15', 'up', '!!!!!!!!\".', 'Y', '`]', 'ZK', ']``', 'Z', '>\"!!!!!!!', '\\n', '14', 'up', '!!!!!!!!!!#.', 'M', '``', 'a', '``', 'Z', ')!!!!!!!!', '\\n', '13', 'up', '!!!!!!!!!!!!!<', '???', 'S', '`', 'Q', '!!!!!!!!', '\\n', '12', 'up', '!!!!!!!!!!!!!!!!!', 'H', '``', '0', '!!!!!!!', '\\n', '11', 'up', \"!!!!!!!!!'\", 'A', '/&!!!!', ':``', '0', '!!!!!!!', '\\n', '10', 'up', '!!!!!!!!!', 'N', '``', 'XJ', '(!!', '4', '``', '0', '!!!!!!!', '\\n', '09', 'up', '!!!!!!!!!', '9', '````', 'W', '(!', '*``', '0', '!!!!!!!', '\\n', '08', 'up', '!!!!!!!!!$', 'G', '\\\\```', 'W', '(', '*``', '0', '!!!!!!!', '\\n', '07', 'up', '!!!!!!!!!!!+', 'G', '^``', 'YU', '`[+!!!!!!!', '\\n', '06', 'up', '!!!!!!!!!!!!!)', 'FU', '```', 'E', '!!!!!!!!', '\\n', '05', 'up', \"!!!!!!!!!!!!!!!$'\", ':?', '\"!!!!!!!!', '\\n', '04', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '03', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '02', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '01', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '\\n', '00', 'up', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(r['text'].replace(' ', '')).ids\n",
    "\n",
    "print(len(token_ids))\n",
    "print([tokenizer.decode([tkn_id]) for tkn_id in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 840\n",
      "201\n",
      ">>> 840\n",
      "201\n",
      ">>> 840\n",
      "233\n",
      ">>> 840\n",
      "233\n",
      ">>> 840\n",
      "201\n",
      ">>> 840\n",
      "201\n",
      ">>> 840\n",
      "176\n",
      ">>> 840\n",
      "176\n",
      ">>> 840\n",
      "195\n",
      ">>> 840\n",
      "195\n",
      ">>> 840\n",
      "223\n",
      ">>> 840\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "for i, r in enumerate(dataset['train']):\n",
    "    print('>>>', len(r['text'].split()))\n",
    "    token_ids = tokenizer.encode(r['text'].replace(' ', '')).ids\n",
    "    print(len(token_ids))\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('cache/tokenizer_simple.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
